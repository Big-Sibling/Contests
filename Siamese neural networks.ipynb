{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-shot learning : The act of learning from oe or a few number of training examples per class. 즉, 하나의 클래스에 한장 또는 몇 장의 사진만으로 \n",
    "class를 러닝하는 방법.\n",
    "\n",
    "one-shot learning을 구현하는 두가지 방법.\n",
    "1. Use meta-learning \n",
    "2. Train robust feature learning model. 그 중 샴 네트워크는 로버스트 피쳐 러닝 모델을 사용함.\n",
    "\n",
    "기존 네트워크들을 사용하면 어째서 One-shot learning을 사용할 수 없을까?\n",
    "1. 기존 네트워크 - 과적합. 예를 들어 기존 네트워크들은 복잡한 구조를 가지고 있는데, 신경망이 고양이라는 개념을 몰랐을 때 고양이 사진을 한장 주면 이 네트워크가 학습을 할 수 없게 됨. 즉, 기존의 class에 대해 과적합이 되어있음.\n",
    "- 여기서 과적합(over fitting)이 무엇인가? : 모델이 훈련 데이터에 대해 너무 잘 훈련되고, 훈련 세트에 대해서 너무 구체적이어서 보이지 않는 새로운 데이터에 대한 성능이 저하될 때 발생한다.\n",
    "일반적인 딥러닝 네트워크는 파라미터가 너무 많아서 one-shot이 아닌 hundred-shot을 해도 오버피팅이 일어남.\n",
    "2. 수 많은 파라미터에 미미한 파동만 줄 수 있음.\n",
    "\n",
    "One-shot learning의 Siamese neural networks은 robust feature를 학습시키는것이 목표이다.\n",
    "신경망의 가장 큰 장점은 고차원 데이터의 중요한 feature를 잘 잡아낸다는 것이다.\n",
    "\n",
    "학습 과정에서 유용한 feature를 잡아내서 one-shot task에 활용한다면? \n",
    "예를 들어 축구공 사진을 넣어놓고 축구공과 축구공 이미지가 같음을 학습시킨 후, 농구공 사진으로 test를 진행했을 때, 북극곰, 야구공, 농구공 중에서 가장 비슷한 이미지를 정답으로 맞추게 함.\n",
    "\n",
    "Omniglot dataset -> one-shot learning에서 가장 유용하게 쓰이는 데이터셋\n",
    "- 50가지 언어\n",
    "- 1623개의 문자, 20명의 필기\n",
    "- 105 x 105 pixel\n",
    "- 30개의 언어로 학습, 20개의 언어로 test\n",
    "\n",
    "N-way one-shot task -> One-shot learning 평가 지표\n",
    "- one-shot learning 평가 방법\n",
    "- [Test image, Support set]\n",
    "- support set에는 test inage와 같은 클래스의 이미지를 한개만 넣음\n",
    "\n",
    "Baseline\n",
    "\n",
    "1-Nearest Neighbor\n",
    "- 가장 간단한 베이스라인\n",
    "- 테스트 이미지에서 유클리디언 거리가 가장 가까운 이미지를 선택\n",
    "- 20-way one-shot task에서 28% 정확도\n",
    "\n",
    "HBPL (Hierachical Bayesian program Learning)\n",
    "- 문자 그릴 때의 획 정보(사전 분포)를 사용하는 생성 모델\n",
    "- 20-way one-shot task에서 95.2%의 정확도\n",
    "- 획에 대한 정보가 필요하다는 단점(구하기가 쉽지 않은 데이터)\n",
    "\n",
    "Network architecture\n",
    "\n",
    "Siamese Net\n",
    "- 4 conv layers\n",
    "- 2 max pooling\n",
    "- FC layer\n",
    "- 각 4096개의 feature의 각 네트워크의 L1 norm을 계산\n",
    "- 그 후 FC layer를 거쳐 sigmoid로 단일 값을 받아냄\n",
    "- 1이면 같은 클래스, 0이면 다른 클래스로 판단\n",
    "- FC layer가 96%의 파라미터를 차지\n",
    "\n",
    "여기서 FC Layer가 무엇인가? : Siamese 네트워크의 FC 계층은 두 입력 샘플을 비교하기 전에 최종 계층 역할을 합니다. FC 계층은 하위 네트워크에서 학습한 기능을 결합하고 두 입력 샘플 간의 유사성 점수 또는 거리 메트릭을 나타내는 단일 출력 값을 생성하는 역할을 합니다.\n",
    "\n",
    "\n",
    "<img src=\"https://blog.kakaocdn.net/dn/cYlVDk/btqCuZqVfbl/ywtxKkwyP7ohAeJ0y5mEK0/img.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "전체적인 예측값과 타겟이 같은지 다른지에 대한 label값을 가지고 binary cross entropy loss를 구한다.\n",
    "\n",
    "* Experiments\n",
    "\n",
    "Training\n",
    "- training image를 한 네트워크에 입력\n",
    "- 다른 네트워크에는 Support set에 있는 이미지를 하나씩 입력\n",
    "- 30개 언어, 964개의 알파벳, 각각 다른 20개의 version(183,549,560)\n",
    "- Augmentation을 위한 affine distortion(같은 글자더라도 조금씩 다르게 생긴 글자를 입력해줌. affine transforming)\n",
    "\n",
    "\n",
    "Test\n",
    "- Test image를 한 네트워크에 입력\n",
    "- 다른 너트워크에는 Support set에 있는 이미지를 하나씩 입력\n",
    "- 왼쪽은 omniglot dataset\n",
    "- 오른쪽은 MNIST dataset\n",
    "\n",
    "<img src=\"https://blog.kakaocdn.net/dn/cpySSL/btqTlFKfJUn/FvxVXI3tj5g12tzWwiLKI0/img.png\" width=\"700\" height=\"400\">  \n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTEQXt56rTFZ-PmVeXgIfHPs4d6C78OM2VkbPaJRH7b_FOTNetZ4lJPq8PyFDGIHVMD2Q&usqp=CAU\" width=\"700\" height=\"400\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC layer에 대한 추가 설명 및 종류와 활용도\n",
    "\n",
    "완전 연결(FC) 계층은 CNN(컨볼루션 신경망) 및 DNN(심층 신경망)을 비롯한 많은 신경망 아키텍처의 중요한 구성 요소입니다. FC 레이어는 각 뉴런이 이전 레이어의 모든 뉴런에 연결되어 네트워크가 입력과 출력 간의 복잡한 비선형 관계를 학습할 수 있도록 하는 레이어 유형입니다.\n",
    "\n",
    "\n",
    "신경망 아키텍처에서 사용할 수 있는 여러 유형의 FC 계층이 있으며 각각 고유한 특성과 응용 프로그램이 있습니다. 아래에서는 FC 레이어의 가장 일반적인 유형과 용도에 대해 설명합니다.\n",
    "\n",
    "\n",
    "* 표준 FC 계층: 표준 FC 계층은 훈련 중에 학습되는 가중치 행렬과 편향 항으로 구성됩니다. 계층에 대한 입력은 이전 계층의 활성화 벡터이고 출력은 다음 계층으로 전달되는 활성화 벡터입니다. 표준 FC 계층은 일반적으로 이미지 분류, 자연어 처리 및 음성 인식과 같은 다양한 작업을 위해 신경망 아키텍처에서 사용됩니다.  \n",
    "* 드롭아웃 레이어: 드롭아웃 레이어는 신경망에서 과적합을 방지하는 데 사용되는 정규화 기술입니다. 교육 중에 드롭아웃 레이어는 레이어의 뉴런 일부를 임의로 드롭아웃하여 네트워크가 데이터의 보다 강력한 표현을 학습하도록 합니다. 드롭아웃 레이어는 일반적으로 이미지 분류 및 기타 컴퓨터 비전 작업을 위한 신경망 아키텍처에서 사용됩니다.  \n",
    "* 배치 정규화 계층: 배치 정규화 계층은 이전 계층의 활성화를 정규화하는 데 사용되어 내부 공변량 이동의 영향을 줄이고 훈련 중에 네트워크의 안정성을 향상시킵니다. 배치 정규화 계층은 일반적으로 이미지 분류, 객체 감지 및 기타 컴퓨터 비전 작업을 위한 신경망 아키텍처에서 사용됩니다.  \n",
    "* Maxout 레이어: maxout 레이어는 모든 입력에 단일 활성화 함수를 적용하는 대신 입력 집합에 대해 최대값을 취하는 활성화 함수 유형입니다. Maxout 레이어는 이미지 분류 및 비선형 변환이 필요한 기타 작업을 위해 신경망 아키텍처에서 일반적으로 사용됩니다.  \n",
    "* 연결 계층: 연결 계층은 두 개 이상의 이전 계층의 출력을 단일 텐서로 결합하는 데 사용됩니다. 연결 레이어는 이미지 분할을 위한 신경망 아키텍처에서 일반적으로 사용되며, 여기에서 여러 컨벌루션 레이어의 출력이 결합되어 분할 마스크를 생성합니다.  \n",
    "\n",
    "전반적으로 FC 계층은 입력과 출력 간의 복잡한 관계를 학습할 수 있는 신경망 아키텍처를 구축하기 위한 강력한 도구입니다. 다양한 유형의 FC 계층을 사용하여 연구원과 엔지니어는 네트워크를 특정 작업에 맞게 조정하고 다양한 기계 학습 작업에서 성능을 향상할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
